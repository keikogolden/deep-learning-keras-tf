{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f311488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install 'spacy~=3.2.6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b08ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install 'spacy~=3.3.3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19304307",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U pydantic spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ca7fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c93f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a366ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fro the past script as a reference.\n",
    "\n",
    "\n",
    "#import re\n",
    "#\n",
    "#sing_plur = []\n",
    "#for skill in df['SkyHive Skill'].tolist():\n",
    "#    \n",
    "#    doc = nlp(skill)\n",
    "#    last_token = doc[len(doc)-1]\n",
    "#    regex = r'''[a-zA-Z-\\.]+([A-Z]+[a-z]*)+''' #This matches with camelcase\n",
    "#    match = re.search(regex, last_token.text)\n",
    "#    \n",
    "#    if match: #catch camelcased product name\n",
    "#        sing_plur.append('')\n",
    "#    elif last_token.pos_=='VERB' or last_token.pos_ == 'ADV' or last_token.pos_ == 'ADJ'or last_token.pos_ == 'PRON': #catch non nouns\n",
    "#        sing_plur.append('')\n",
    "#    elif last_token.text != last_token.text.title() and last_token.text != last_token.lemma_: #catch plural nouns\n",
    "#        sing_plur.append('plur')\n",
    "#    elif last_token.text == last_token.lemma_: # catch singular nouns\n",
    "#        sing_plur.append('sing')     \n",
    "#    else:\n",
    "#        sing_plur.append('') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca08746",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"C:/Users/KeikoGolden/ComputationalLiguistics/KE714_extract_NN_PRON_to add_context/KE714_deliverable_using_review.xlsx\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b6887a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count how many bigrams present in the df.\n",
    "\n",
    "\n",
    "bigrams = []\n",
    "\n",
    "for skill in df.skill:\n",
    "    doc = nlp(skill)\n",
    "    \n",
    "    if len(doc) == 2:\n",
    "        bigrams.append(doc)\n",
    "\n",
    "len(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a097a527",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTION1: Extracting annotation data by focusing on bigrams.\n",
    "\n",
    "need_review = []\n",
    "bigrams = []\n",
    "\n",
    "for skill in df.skill:\n",
    "    doc = nlp(skill)\n",
    "    first_token = doc[0]\n",
    "    last_token = doc[len(doc)-1]\n",
    "\n",
    "    \n",
    "    if len(doc) == 2:\n",
    "        bigrams.append(True)\n",
    "        if first_token.pos_ == 'VERB' or first_token.pos_ == 'ADV':\n",
    "            need_review.append(False)\n",
    "        elif last_token.pos_ == 'VERB' or last_token.pos_ == 'ADV':\n",
    "            need_review.append(False)\n",
    "        else:\n",
    "            need_review.append(True)\n",
    "    else:            \n",
    "        bigrams.append(False)\n",
    "        need_review.append(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdda48a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tagging unigrams in df.\n",
    "unigrams = []\n",
    "\n",
    "for skill in df.skill:\n",
    "    doc = nlp(skill)\n",
    "    if len(doc)==1:\n",
    "        unigrams.append(True)\n",
    "    else:\n",
    "        unigrams.append(False)\n",
    "\n",
    "df['unigram'] = unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a14db5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"C:/Users/KeikoGolden/ComputationalLiguistics/KE714_extract_NN_PRON_to add_context/test_output_with_bigrams_unigrams.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d01409",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bigram'] = bigrams\n",
    "df['need_reivew'] = need_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3c025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"C:/Users/KeikoGolden/ComputationalLiguistics/KE714_extract_NN_PRON_to add_context/test_output_with_bigrams.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8d7723",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTION2: Extracting annotation data for all ngrams.\n",
    "\n",
    "need_review = []\n",
    "for skill in df.skill:\n",
    "        \n",
    "    doc = nlp(skill)\n",
    "    first_token = doc[0]\n",
    "    last_token = doc[len(doc)-1]\n",
    "    \n",
    "    if first_token.pos_ == 'VERB' or first_token.pos_ == 'ADV':\n",
    "        need_review.append(False)\n",
    "    elif last_token.pos_ == 'VERB' or last_token.pos_ == 'ADV':\n",
    "        need_review.append(False)\n",
    "    else:\n",
    "        need_review.append(True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfedb874",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['need_reivew'] = need_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae3c5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(\"C:/Users/KeikoGolden/ComputationalLiguistics/KE714_extract_NN_PRON_to add_context/test_output.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a340da",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"event planning\")\n",
    "for token in doc:\n",
    "    print(token.lemma_, \"| \", token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9711e4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"planning an event\")\n",
    "for token in doc:\n",
    "    print(token.lemma_, \"| \", token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714fff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"planning events\")\n",
    "for token in doc:\n",
    "    print(token.lemma_, \"| \", token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8429df",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"up-selling\")\n",
    "for token in doc:\n",
    "    print(token.lemma_, \"| \", token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd001c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"up selling\")\n",
    "for token in doc:\n",
    "    print(token.lemma_, \"| \", token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bcec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"panel building\")\n",
    "for token in doc:\n",
    "    print(token.lemma_, \"| \", token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51addc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"containment\")\n",
    "for token in doc:\n",
    "    print(token.lemma_, \"| \", token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e6ad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"natural gas\")\n",
    "for token in doc:\n",
    "    print(token.lemma_, \"| \", token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6825f406",
   "metadata": {},
   "source": [
    "# allocate files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06c5a51",
   "metadata": {},
   "source": [
    "##### To remove context in parenthesis from the skill\n",
    "    =IF(ISBLANK(E2), C2, LEFT(C2, (SEARCH(\"(\", C2)-1)))\n",
    "\n",
    "    =IF(ISBLANK(F2), D2, LEFT(D2, (SEARCH(F2, D2)-2)))\n",
    "    \n",
    "##### * Make sure to remove trailing spaces from each skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dd5c1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(\"C:/Users/KeikoGolden/ComputationalLiguistics/KE714_extract_NN_PRON_to add_context/add_context_bigrams_unigrams.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd9be5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "028a1c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "portion = round(len(df)/11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "933a1df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[:portion].copy()\n",
    "df2 = df[portion:portion*2].copy()\n",
    "df3 = df[portion*2:portion*3].copy()\n",
    "df4 = df[portion*3:portion*4].copy()\n",
    "df5 = df[portion*4:portion*5].copy()\n",
    "df6 = df[portion*5:portion*6].copy()\n",
    "df7 = df[portion*6:portion*7].copy()\n",
    "df8 = df[portion*7:portion*8].copy()\n",
    "df9 = df[portion*8:portion*9].copy()\n",
    "df10 = df[portion*9:portion*10].copy()\n",
    "df11 = df[portion*10:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ab05020",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_excel(\"C:/Users/KeikoGolden/ComputationalLiguistics/KE714_extract_NN_PRON_to add_context/add_context_bigrams_unigrams_Anay.xlsx\")\n",
    "df2.to_excel(\"C:/Users/KeikoGolden/ComputationalLiguistics/KE714_extract_NN_PRON_to add_context/add_context_bigrams_unigrams_Cassady.xlsx\")\n",
    "df3.to_excel(\"C:/Users/KeikoGolden/ComputationalLiguistics/KE714_extract_NN_PRON_to add_context/add_context_bigrams_unigrams_Desiree.xlsx\")\n",
    "df4.to_excel(\"C:/Users/KeikoGolden/ComputationalLiguistics/KE714_extract_NN_PRON_to add_context/add_context_bigrams_unigrams_Ethan.xlsx\")\n",
    "df5.to_excel(\"C:/Users/KeikoGolden/ComputationalLiguistics/KE714_extract_NN_PRON_to add_context/add_context_bigrams_unigrams_Craig.xlsx\")\n",
    "df6.to_excel(\"C:/Users/KeikoGolden/ComputationalLiguistics/KE714_extract_NN_PRON_to add_context/add_context_bigrams_unigrams_Darrell.xlsx\")\n",
    "df7.to_excel(\"C:/Users/KeikoGolden/ComputationalLiguistics/KE714_extract_NN_PRON_to add_context/add_context_bigrams_unigrams_Hemu.xlsx\")\n",
    "df8.to_excel(\"C:/Users/KeikoGolden/ComputationalLiguistics/KE714_extract_NN_PRON_to add_context/add_context_bigrams_unigrams_Mariam.xlsx\")\n",
    "df9.to_excel(\"C:/Users/KeikoGolden/ComputationalLiguistics/KE714_extract_NN_PRON_to add_context/add_context_bigrams_unigrams_Eugene.xlsx\")\n",
    "df10.to_excel(\"C:/Users/KeikoGolden/ComputationalLiguistics/KE714_extract_NN_PRON_to add_context/add_context_bigrams_unigrams_Keiko.xlsx\")\n",
    "df11.to_excel(\"C:/Users/KeikoGolden/ComputationalLiguistics/KE714_extract_NN_PRON_to add_context/add_context_bigrams_unigrams_Sazzad.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e47592",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
